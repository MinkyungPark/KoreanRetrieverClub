# 📚 7 - 3 주차

### 주제 1 : PLAN-AND-ACT : Improving Planning of Agents for Long-Horizon Tasks
- **발표자**: 박민경

### 리뷰 1
- **리뷰어**: 
(내용)

---

### 리뷰 2
- **리뷰어**: 
(내용)

---

<br>

### 주제 2 : DiarizationLM : Speaker Diarication Post-Processing with Large Language Models
- **발표자**: 장재후

### 리뷰 1
- **리뷰어**: 박민경

speaker diarization의 후처리에 LLM을 도입해 ASR과 diarization 결과를 오케스트레이션 한 뒤 word-level에서 발생하는 화자 라벨 오류를 교정하는 접근법에 대한 발표. 입력으로 단어와 화자 라벨을 결합한 compact prompt를 구성하고 LLM이 화자 라벨링이 수정된 결과를 제공한다. 이 출력을 기반으로 파인튜닝하거나 few-shot하여 word-level 화자 라벨링 오류를 교정하는 것 같다. 초기에 다소 의아했던 것은 LLM 추론능력만으로 단어 기반 문맥 정보로 화자 라벨을 교정할 수 있다고 가정한 부분이었다. 특히 음향 특징을 사용하지 않고 텍스트만으로 화자 교정을 한다는 점이 정확도가 떨어지지 않을까 생각했다. 실험에서는 WDER이 유의미하게 줄어들었다고 한다. 문맥만으로 명확하게 판단 가능한 오류에서는 상당한 보정 효과가 있는 것 같다.

---

### 리뷰 2
- **리뷰어**: 
(내용)

---

<br>

### 주제 3 : Emergent Misalignment : Narrow finetuning can produce broadly misaligned LM
- **발표자**: 조재현

### 리뷰 1
- **리뷰어**: 박민경

보안에 취약한 코드 생성과 같이 한정된 도메인에서 이상한 데이터로 파인튜닝하는 경우에도 LLM에서 광범위한 Misalignment 문제가 야기될 수 있음을 보여준 논문에 대한 발표. 의도적으로 취약한 코드만 생성하게 하면 전혀 관련 없는 일반적인 질문에 대해서도 위험하고 Unsafe한 답변을 하는 emergent behavior를 보였다는 점이 굉장히 흥미로웠다. 특히 보안 취약점이 있는 코드를 출력하도록 하는 데이터셋이라도 좋은 의도임을 밝히는 프롬프트를 붙이면 Misalignment 문제가 억제되는 것 또한 재미있었다. 즉, 모델은 단순히 무엇을 학습했나보다 데이터의 맥락과 의도가 alignment 유지에 더 중요한 역할을 한다는 것을 알 수 있었다. 그 외에도 LLM hacking에 Jailbreak, Backdoor에 대한 것과 Grokking 현상에 대해서도 알 수 있었다. 

전반적으로 제한된 학습에서 LLM의 unsafety와 misalignment에 미치는 영향을 구체적인 실험을 근거로 입증하고 있으며 재현씨가 이를 자세하게 설명해주어 이해하기도 쉬웠다. 결론적으로는 학습 데이터의 '의도'라는 정성적 변수가 misalignment에 핵심적인 역할을 한다는 점이 신선했다. 이에 대한 구체적인 내부 메커니즘은 명확하게 설명되진 않았고 LLM이 실제 배포될 때 여러 해킹 기법이나 Unsafety 탐지를 어떻게 보완할 것인지에 대한 연구들은 부족한 것 같다. 트리거 기반의 오작동(backdoor) 문제 등은 Mesa-optmizaion 분야외도 맞물려 있는 문제이기도 하다. 해당 논문에서 다루고 있는 Emergent misalignment가 mesa optmization이 실제로 존재할 가능성을 뒷받침한다고도 볼 수 있을 것 같다. 

---

### 리뷰 2
- **리뷰어**: 
(내용)

---
