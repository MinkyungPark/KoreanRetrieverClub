# 📚 - 주차

### 주제 1: Spurious Rewards: Rethinking Training Signals in RLVR
- **발표자**: 박민경

### 리뷰 1
- **리뷰어**: 장재후

RLVR 개념   
RLVR은 언어 모델을 강화학습으로 훈련하는 프레임워크며 수학 문제, 코드 실행 검증 등 정답이 deterministic한 영역의 과제에 적용하기 적합하다. RLHF와의 차이는 RLHF의 경우 사람의 정성 피드백을 reward로 주지만 RLVR은 자동화된 demonstrator가 존재하고 이걸로 reward를 책정해서 주는 방식이다. SFT와 비교해보면 SFT는 문제와 정답을 알려주고 직접 가르치는것이다. 복잡한 추론이 필요한 경우, 해결방안을 이해하는 것 또한 어렵기 때문에 SFT를 적용하기 힘들다고 한다. 이에 비해 RLVR은 정답에 대한 reward signal만 준다고 한다. 즉, 다양한 풀이 방식을 훈련하게 하고 이에 대한 reward를 줘서 문제 해결과정까지 터득하게 한다.

주제   
논문의 골자는 RLVR 적용시 거짓 raward를 주더라도, 특정 모델에서는 수학적 추론 능력을 유도할 수 있다는 것이다. 여기서 거짓 reward라는 것은 랜덤 reward가 될 수도있고, 정답 형식만 갖춰도 보상을 준다던가, 다른 라벨에 보상을 준다는 던가 다양한 방식이 존재한다. 

후기   
모델이 훈련한 데이터 분포가 다양하면 RLVR 기법이 잘먹힐 것 같다는 생각이 들었다. 또한 reasoning 출력을 내는 여러 task들에 적용이 가능해보인다는 생각이 들었다. RLVR이 코드추론을 유도해서 코드 reasoning을 통해서 성능이 증가되는것 같다는 느낌도 들었다. Qwen모델같은 경우 전체 응답 중 65%를 파이썬 코드를 중간 과정으로 생성하는 방식으로 출력을 한다고 하는데 Credit assginment가 명확하게 존재하지 않는 foundation 모델의 각 task들에도 RLVR 기법을 적용할 수 있는 방법을 찾는게 향후 연구 주제일 것 같다.


---

### 리뷰 2
- **리뷰어**: 
(내용)

---

<br>

### 주제 2: Absolute Zero: Reinforced Self-play Reasoning with Zero Data
- **발표자**: 조재현

### 리뷰 1
- **리뷰어**: 박민경

1. 문제의식

추론/코딩 모델은 사람이 생성한 데이터셋에 기반해 학습된다. 하지만 인간이 설계한 문제는 비용이 높고 편향될 수도 있으며 범위가 제한적. 또한 모델은 주어진 데이터셋을 과적합하게 되고 일반적인 추론 능력을 키우기가 어렵다. 또한 인간이 만든 데이터셋으로는 모델 성능의 고점이 정해진다는 한계가 존재한다.

2. Absolute Zero

이 발표에서는 모델이 스스로 문제를 만들고 푸는 방식으로 추론 능력을 강화하는 'self-play reasoning' 기법을 소개한다. 랜덤 씨드로써 가장 최초의 아주 간단한 문제 1개는 필요하지만 그 외에 외부 문제 데이터나 사람의 피드백이 전혀 필요하지 않고 보상도 코드 실행 결과로 RLVR을 통해 줄 수 있다. 

3. 문제 생성

문제를 만들 때 program-input을 주고 Output을 생성, program-output을 주고 Input을 생성, input-output을 주고 Program을 생성하는 3가지 구조로 나눴다. 이 구조를 바탕으로 모델은 논리적으로 타당한 문제 구조를 학습할 수 있고 점점 복잡한 문제를 생성할 수 있게 되는 것 같다.

4. 리뷰

학습 데이터셋이 없어도 Absolute zero 방식은 수학, 코딩 문제 해결 능력에서 강력한 성능향상이 있었다. 이는 사람이 데이터셋을 생성하는 기존 방법론의 한계를 극복할 수 있게 한다. Absolute zero은 문제 생성자와 해결자 역할을 동시에 수행하면서 문제를 잘 만드는 것이 곧 추론 능력을 학습하는 핵심이 될 수 있다는 점을 실험적으로 보였다. 인간의 개입이 없는 스스로 사고하는 인공지능을 위한 중요한 연구 내용이 될 것 같다.

---

### 리뷰 2
- **리뷰어**: 장재후

Absolute Zero   
외부 데이터에 전혀 의존하지 않고, 단일 모델이 스스로 학습 진행을 최대화하는 태스크를 제안하고 이를 해결하며 추론 능력을 향상시키는 프레임워크다. 이는 기존 RLVR 방식의 데이터 의존성 한계를 극복할 수 있다. AZR은 코드 실행기를 환경으로 사용하여 스스로 학습 커리큘럼과 추론 능력을 발전시킨다. 코드 실행기는 제안된 코드 추론 태스크의 유효성을 검증하고 답변의 정확성을 확인하는 데 사용된다. 또한  verifiable reward의 통합적인 소스 역할을 한다. Deduction, Abduction, Induction이라는 3가지 종류의 입, 출력 예시 및 프로그램을 직접 생성하고 훈련한다. 실험 결과, AZR은 외부 큐레이션된 데이터 없이 학습되었음에도 불구하고, HumanEval+, MBPP+, LiveCodeBench 등의 코딩 벤치마크와 AIME, OlympiadBench, Minerva, Math500, AMC 등의 수학 벤치마크에서 기존 zero setting 모델들을 능가하는 SOTA 성능을 달성했다. 특히 코더 기반 모델은 일반 기반 모델보다 더 높은 성능 향상을 보였으며, 모델 크기가 클수록 성능 향상 폭이 커지는 경향이 관찰되었다. 

후기   
데이터를 직접 생성해서 훈련에 활용하고 성능을 높이는 방안이 모달리티 상관없이 적용될 수 있다면 데이터 수집 비용 절감에 혁신적인 방안이 될 수 있을 것이라는 생각이 든다. 텍스트 뿐만아니라 음성, 이미지도 generation 모델 기반으로 위와 같은 method들을 적용할 수 있는 방안을 찾는다면 좋을 것 같다.

---

<br>

### 주제 3: Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down
- **발표자**: 장재후

### 리뷰 1
- **리뷰어**: 박민경

1. 문제 정의

위스퍼는 뛰어는 음성인식 성능을 보이는 모델이지만 노이즈(배경음, 소음)등에 대해 실제 존재하지 않는 문장을 생성하는 hallucination문제가 있다. 특히 whisper는 autoregressive decoder 구조를 가지고 있기 때문에 인코더만을 사용하는 CTC계열 모델 대비 hallucination문제에 훨씬 취약하다고 볼 수 있다.

2. 해결 방법

이 세미나의 발표 내용에서는 whisper의 hallucination문제가 디코더의 self-attention 헤드에 의거한 것이라고 보고 이에 대한 영향을 분석했다. 그리고 몇 개의 헤드에서 전체 hallucination현상의 대부분을 유발하는 현상을 관찰했다. 이에 착안하여 식별된 몇 개의 문제 헤드만을 노이즈 데이터셋으로 따로 파인튜닝하여 hallucination을 줄이는 방법을 제안하였다.

3. 리뷰

풀 파인튜닝 혹은 몇 개의 헤드만 파인튜닝 하는 경우에도 hallucination현상이 줄어들면 WER이 증가하는 trade-off 결과를 관찰할 수 있었다. 그래도 여기서 주장한 Calm-down 기법은 hallucination를 크게 감소시키면서 WER이 조금 상승하는 결과를 보여주어 성능 저하 없이 hallucination문제를 효과적으로 완화하였다고 한다. 위스퍼의 Seq2Seq 구조에서 오는 이러한 문제점들을 분석하여 해결하는 연구들이 앞으로 더 기대가 된다.

---

### 리뷰 2
- **리뷰어**: 
(내용)

---
