# 📚 - 주차

### 주제 1: Large Language Diffusion Models
- **발표자**: 박민경

### 리뷰 1
- **리뷰어**: 조재현


---

### 리뷰 2
- **리뷰어**: 장재후


---

<br>

### 주제 2: DeepScaleR: Surpassing O1-preview with a 1.5B Model by Scaling RL
- **발표자**: 조재현

### 리뷰 1
- **리뷰어**: 박민경

- Review :
```
Deepseek 논문에서 큰 모델으로만 RL로 성능을 낼 수 있다에 대한 주장에 대해 작은 모델로도 RL로 대형 모델의 성능을 뛰어 넘을 수 있다는 것을 보여준 내용. 
Qwen모델을 distilation하여 만든 deepseek-rl경량 모델을 기초 모델로 사용하였다. 즉 대형 모델을 distil하여 지식이 충분히 축적된 상태에서 RL을 적용했다는 것이 핵심인 것 같다. 

이 세미나에서 인상 깊었던 점은 아래와 같다.

모델의 크기보다 중요한 것은 pre-train의 질, 보상 설계, context 적응 능력, 그리고 안정적인 RL 알고리즘 설계이다.
DeepScaleR은 단순히 작은 모델로 실험한 것이 아니고 
    - 티쳐 모델로부터 정제된 지식을 여러 스킬을 통해 효과적으로 distil하고,
    - 적절한 보상 체계와 안정적인 RL(GRPO)로 학습을 하였으며,
    - 문맥 길이를 단계적으로 증가시키는 전략을 채택하는 등으로, 
전반적으로 학습 전략이 정교하게 설계된 결과인 것 같다.
이번 세미나를 통해서 RL이 대형 모델에만 적용될 수 있다기보단 고품질 Pre-train 지식이 주입된 사전 모델이 down stream task 성능에 핵심이라는 것을 또 한번 느끼게 되었다.
```

---

### 리뷰 2
- **리뷰어**: 장재후


---

<br>

### 주제 3: Chain-of-Thought Prompting for Speech Translation
- **발표자**: 장재후

### 리뷰 1
- **리뷰어**: 박민경

- Review :
```
음성 번역에서 기존에는 음성을 바로 번역하거나, Cascade 방식(음성 -> ASR -> MT)를 사용했으며 Casecade 방식이 성능면에서는 더 뛰어나지만 ASR 결과가 부정확하다면 MT 성능까지 영향을 미칠 수 있다는 단점이 있었다. 이 세미나에서 제안한 방식은 CoT prompting을 도입하고 ASR 결과 자체를 Prompt에 도입시켜 LLM이 더 좋은 번역을 생성하도록 하였다.
다만, 실험 결과만 보면 단순히 ASR 결과를 LLM에 입력해 번역하는 방식과의 성능 차이는 크지 않았다. 인코더를 재학습하고 프롬프트 엔지니어링을 추가적으로 도입하는 방식은 성능 면에서는 일정 수준의 향상을 기대할 수 있지만, 복잡도, 개발 비용, 유지 보수 측면에서는 부담이 클 수 있다. 반면, 기존 ASR 시스템의 출력을 받아 단순히 LLM을 이용해 번역만 수행하는 방식은 구조가 단순하고 구현 및 확장에 유리하여 실용적인 측면에서는 여전히 매력적인 선택지이다. 그럼에도, 이 논문은 추가적으로 음성 인코더 구조와 입력 전략(prompt 구성방식)에 따라 성능 차이가 발생함을 실험적으로 보여주었다는 점에서 중요한 통찰을 제공하고, 음성 번역에서 모델 간 추론 작업에 CoT를 적용한 사례라는 것은 학술적 의의를 주는 것 같다.
```

---

### 리뷰 2
- **리뷰어**: 조재현


---
