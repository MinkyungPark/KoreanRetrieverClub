# 📚 5 - 4 주차

### 주제 1: Does fine-tuning llms on new knowledge encourage hallucinations
- **발표자**: 조재현

### 리뷰 1
- **리뷰어**: 장재후

### 요약
```
LLM의 할루시네이션은 완전 잘 잡히지 않으며 finetuning하면서부터 발생함.
할루시네이션에 대해 알려진 사실들은, 새로운 지식을 학습하면 생긴다는 의견이 많음.
새로운 지식을 학습하는게 LLM의 할루시네이션 발생에 어떻게 영향을 주는지 분석한 논문임.
Unknown example의 부정적 영향은 주로 finetuning 후기에서 나타났으며, MaybeKnown example의 활용이, 모델이 기존 지식을 효과적으로 활용하는 데 필수적임이 확인되었음.
```

### 후기
```
할루시네이션의 발생 원인을 훈련 데이터 및 훈련 시점 측면에서 파악하려고 시도 한 논문이었음.
데이터를 활용해 가시적으로 수치화하여 환각 현상의 원인을 파악하려는 논문을 처음 봐서 신기했음.
LLM 뿐 만 아니라 각종 모달리티들의 생성모델에서 발생하는 환각 현상도 설명할 수 있는 방법을 찾으면 좋은 연구가 될것으로 생각됨. 
```
---

### 리뷰 2
- **리뷰어**: 
(내용)
---

<br>

### 주제 2: SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information
- **발표자**: 장재후

### 리뷰 1
- **리뷰어**:
(내용)
---
### 리뷰 2
- **리뷰어**: 
(내용)
---

<br>

### 주제 3: A Bayesian approach for prompt optimization in pretrained language models
- **발표자**: 박민경

### 리뷰 1
- **리뷰어**: 장재후
### 요약
```
언어 모델(PLM)에서 프롬프트 최적화 문제를 베이지안 최적화(BO)를 활용하여 해결한 논문임.
프롬프트 최적화 문제를 블랙박스 문제로 바라보고, 그래디언트 정보 없이 discrete token을 직접 찾는 Hard Prompt Tuning 방식에 초점을 맞추며, 이를 찾아낸 discrete space를 continous space에 임베딩하여 BO를 적용함.
실험 결과는 BO가 다양한 작업에서 다른 블랙박스 최적화 방법들과 유사하거나 우수한 성능을 보임을 입증하며, 이산 토큰 공간에서의 BO 효과성을 확인시켜 주었음.
```

### 후기
```
모델 외부 요소가 아닌 임베딩 스페이스를 조작해서 고품질의 프롬프트를 생성하려고 유도한 점이 신기했음.
후속 연구로 이산공간에서 연속공간으로 매핑시 정보 소실을 방지하는 기법을 추가로 적용하면 성능이 더 좋아질 것 같음.
또한 각 토큰의 개별적 특성을 살리면서 내재 표현 공간을 생성하는 방법을 연구해 가우시안 과정에 추가한 뒤, 성능이 어떻게 변하는지 관찰하는것도 좋을 것 같음.
```




---
### 리뷰 2
- **리뷰어**: 
(내용)
---
