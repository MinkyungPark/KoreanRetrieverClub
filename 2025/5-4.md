# 📚 5 - 4 주차

### 주제 1: Does fine-tuning llms on new knowledge encourage hallucinations
- **발표자**: 조재현

### 리뷰 1
- **리뷰어**: 장재후

### 요약
```
LLM의 할루시네이션은 완전 잘 잡히지 않으며 finetuning하면서부터 발생함.
할루시네이션에 대해 알려진 사실들은, 새로운 지식을 학습하면 생긴다는 의견이 많음.
새로운 지식을 학습하는게 LLM의 할루시네이션 발생에 어떻게 영향을 주는지 분석한 논문임.
Unknown example의 부정적 영향은 주로 finetuning 후기에서 나타났으며, MaybeKnown example의 활용이, 모델이 기존 지식을 효과적으로 활용하는 데 필수적임이 확인되었음.
```

### 후기
```
할루시네이션의 발생 원인을 훈련 데이터 및 훈련 시점 측면에서 파악하려고 시도 한 논문이었음.
데이터를 활용해 가시적으로 수치화하여 환각 현상의 원인을 파악하려는 논문을 처음 봐서 신기했음.
LLM 뿐 만 아니라 각종 모달리티들의 생성모델에서 발생하는 환각 현상도 설명할 수 있는 방법을 찾으면 좋은 연구가 될것으로 생각됨. 
```
---

### 리뷰 2

- **리뷰어**: 박민경

1. 발표 내용

LLM은 pre-train에서 다양한 지식을 내재화 한다. 실제 응용에서는 특정 도메인이나 태스크, 또는 최신 지식을 반영하기 위하 fine-tune이 필요하다. 이때 새로운 정보로 fine-tune이 모델의 기존 지식 정합성과 망각 유발 가능성에 대해 어떤 영향을 미치는지에 대한 연구들은 부족했다. 해당 논문은 LLM이 새로운 지식을 학습하는 과정에서 발생할 수 있는 hallucination 문제를 체계적으로 분석했다. LM이 정답을 사전에 알고 있는 정도에 따라 HighlyKnown, MaybeKnown, WeaklyKnown, Unknown으로 지식을 분류하고, 데이터셋을 사용해 Unknown 비중을 조절하며 fine-tune을 수행한 결과 Unknown 예제가 많아질수록 hallucination이 선형적으로 증가하고, MaybeKnown 예제를 적절히 섞는 것이 성능 향상과 안정성 측면에서 가장 효과적이며, 조기 종료는 hallucination을 줄이는 데 유의미한 전략임을 보였다. 

2. 후기

단순히 최신 정보를 주입하기 위해 fine-tuning을 사용하는 것이 오히려 LLM의 기존 지식 기반 추론 능력을 훼손하고 hallucination을 유발할 수 있다는 점을 시사하는 내용이었다. RAG나 편향 보정 전략 같은 대안적 접근을 고려할 필요가 있고, 특히 fine-tuning이 정답을 바르게 기억하게 만들기보다 자신 있게 틀리는 방향으로 모델을 왜곡시킬 수 있다는 점은 실제 응용에서 fine-tuning을 설계할 때 반드시 고려해야 할 핵심 포인트로 느껴졌다.


---

<br>

### 주제 2: SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information
- **발표자**: 장재후

### 리뷰 1
- **리뷰어**: 조재현

### 후기

Audio-Language model에는 기존에 reasoning에 대한 성능을 evaluation하는 benchmark가 부재. 이 논문에서는 benchmark를 제안하고, 기존의 model들을 분석. 인상적인 포인트는, audio 정보를 없애고, 그냥 text화하는 것이 reasoning성능에 더 좋은 영향을 준다는 것이다. Image, text multimodal을 생각해보면 이미지에 대한 정보를 안 넣고 이미지에 대한 설명만 text로 넣는 것이 reasoning에 더 좋다는 건데, 왜 그런지 궁금하긴 하다. (물론 audio-text의 경우 이런 현상이 있고, image-text의 경우엔 이런 현상은 없을 것 같긴 하다.) 생각나는 의심은, audio 자체가 갖고 있는 feature이 분명 text화 했을 때보다 풍부할 건데, 이 feature는 reasoning에는 별로 도움이 안 될 것 같다. 또한, reasoning이라는 task 자체가 lanuage model이 잘하니 그런 것 같기도 하다.

---
### 리뷰 2

- **리뷰어**: 박민경

1. 발표 내용

오디오-언어 모델의 mult-step 추론 능력을 체계적으로 평가하기 위해 SAKURA라는 새로운 벤치마크를 제안했다는 내용이었다. 이는 단순 인식 능력을 넘어 복합적 정보가 필요한 추론을 수행할 수 있는 지표가 된다. 실험 결과 모델들은 텍스트 기반 정보에는 비교적 실제 추론을 한다는 것을 보여주는 결과였지만 오디오 기반 정보에서는 추론에서 성능이 급격하게 저하되었다. 이는 현존하는 오디오-언어모델이 오디오 정보를 통합하여 추론에 사용하는 데는 한계가 있다는 것을 보여준다.

2. 후기

멀티모달 모델 연구가 크게 진행되고 있음에도 음성 영역에서 reasoning 평가가 시급하다는 문제의식이 잘 드러난 발표 내용이었다. 기존 벤치마크들이 모델의 단편적인 능력만을 평가했다면 SAKURA는 추론이라는 복잡한 능력을 오디오 기반으로 체계화 했다는 점에서 의미가 있다. 특히 multi-hop 추론의 성능이 낮다는 점이 인상 깊었는데 향호 오디오-언어 모델의 학습 및 평가 방향에 실제로 영향을 줄 수 있는 기여라고 생각된다. 또한 이 논문을 시작으로 음성에서도 reasoning이 가능하여 음성 태스크 성능을 크게 끌어올릴 수 있을지도 기대가 된다.


---

<br>

### 주제 3: A Bayesian approach for prompt optimization in pretrained language models
- **발표자**: 박민경

### 리뷰 1

- **리뷰어**: 장재후

### 요약
```
언어 모델(PLM)에서 프롬프트 최적화 문제를 베이지안 최적화(BO)를 활용하여 해결한 논문임.
프롬프트 최적화 문제를 블랙박스 문제로 바라보고, 그래디언트 정보 없이 discrete token을 직접 찾는 Hard Prompt Tuning 방식에 초점을 맞추며, 이를 찾아낸 discrete space를 continous space에 임베딩하여 BO를 적용함.
실험 결과는 BO가 다양한 작업에서 다른 블랙박스 최적화 방법들과 유사하거나 우수한 성능을 보임을 입증하며, 이산 토큰 공간에서의 BO 효과성을 확인시켜 주었음.
```

### 후기
```
모델 외부 요소가 아닌 임베딩 스페이스를 조작해서 고품질의 프롬프트를 생성하려고 유도한 점이 신기했음.
후속 연구로 이산공간에서 연속공간으로 매핑시 정보 소실을 방지하는 기법을 추가로 적용하면 성능이 더 좋아질 것 같음.
또한 각 토큰의 개별적 특성을 살리면서 내재 표현 공간을 생성하는 방법을 연구해 가우시안 과정에 추가한 뒤, 성능이 어떻게 변하는지 관찰하는것도 좋을 것 같음.
```




---
### 리뷰 2
- **리뷰어**: 조재현
### 후기
이전과 비슷하게 prompt optimization과 관련한 논문이다. 우선 background인 soft prompt tuning과 hard prompt tuning이란 방식이 있는 것도 처음 알았다. 베이지안 방식을 이용하여, prompt tuning을 하는 것은, 사실 베이지안 최적화에 대해 잘 몰라서 완전하게는 이해하지 못했다. 하지만 결론적으로, hard prompt tuning은 cost를 꽤 신경 써야만 하는데, 이 면에서 효율적인 것 같다.

---
